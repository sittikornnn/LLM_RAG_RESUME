{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1e6c389",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_markdown_file(file_path):\n",
    "    \"\"\"\n",
    "    อ่านเนื้อหาทั้งหมดจากไฟล์ Markdown ที่กำหนด\n",
    "\n",
    "    Args:\n",
    "        file_path (str): เส้นทางไปยังไฟล์ Markdown\n",
    "\n",
    "    Returns:\n",
    "        str: เนื้อหาของไฟล์เป็นข้อความ, หรือ None ถ้าเกิดข้อผิดพลาด\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            content = f.read()\n",
    "            return content\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: The file at {file_path} was not found.\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")\n",
    "        return None\n",
    "\n",
    "# ตัวอย่างการใช้งาน\n",
    "file_name = r'data\\resume.txt'\n",
    "\n",
    "# สมมติว่ามีไฟล์ชื่อ sample_document.md อยู่ใน directory เดียวกัน\n",
    "# สร้างไฟล์ sample_document.md เพื่อทดสอบ:\n",
    "# with open(file_name, 'w', encoding='utf-8') as f:\n",
    "#     f.write(\"# Hello, World!\\n\\nThis is a sample markdown file.\")\n",
    "\n",
    "markdown_content = read_markdown_file(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7eb3c314",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total chunks: 40\n",
      "\n",
      "{'id': 'doc1#chunk1', 'text': '<!-- image -->\\n\\n## ABOUT ME'}\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Create text splitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=150,\n",
    "    chunk_overlap=50,\n",
    "    length_function=len,\n",
    "    is_separator_regex=False,\n",
    ")\n",
    "\n",
    "# Split into chunks\n",
    "texts = text_splitter.create_documents([markdown_content])\n",
    "\n",
    "print(f\"Total chunks: {len(texts)}\\n\")\n",
    "\n",
    "# Convert to JSON-like dict output\n",
    "formatted_output = []\n",
    "\n",
    "for i, doc in enumerate(texts, start=1):\n",
    "    chunk_id = f\"doc1#chunk{i}\"\n",
    "    formatted_output.append({\n",
    "        \"id\": chunk_id,\n",
    "        \"text\": doc.page_content\n",
    "    })\n",
    "\n",
    "# Show first chunk\n",
    "print(formatted_output[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4993472c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total semantic chunks: 8\n",
      "\n",
      "First semantic chunk:\n",
      "{'id': 'doc1#chunk1', 'text': '<!-- image -->'}\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import ExperimentalMarkdownSyntaxTextSplitter\n",
    "\n",
    "# Markdown header rules\n",
    "headers_to_split_on = [\n",
    "    (\"#\", \"Header 1\"),\n",
    "    (\"##\", \"Header 2\"),\n",
    "]\n",
    "\n",
    "# Initialize markdown splitter\n",
    "text_splitter = ExperimentalMarkdownSyntaxTextSplitter(\n",
    "    headers_to_split_on=headers_to_split_on,\n",
    "    return_each_line=False\n",
    ")\n",
    "\n",
    "# Perform markdown-based semantic chunking\n",
    "docs = text_splitter.split_text(markdown_content)\n",
    "\n",
    "print(f\"Total semantic chunks: {len(docs)}\\n\")\n",
    "\n",
    "# Convert to JSON-like output\n",
    "formatted_output = []\n",
    "\n",
    "for i, doc in enumerate(docs, start=1):\n",
    "    chunk_id = f\"doc1#chunk{i}\"\n",
    "    formatted_output.append({\n",
    "        \"id\": chunk_id,\n",
    "        \"text\": doc.page_content.strip()   # Document -> text\n",
    "    })\n",
    "\n",
    "# Show first chunk\n",
    "print(\"First semantic chunk:\")\n",
    "print(formatted_output[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "63aa3acd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': 'doc1#chunk1', 'text': '<!-- image -->'},\n",
       " {'id': 'doc1#chunk2',\n",
       "  'text': 'As a recent Artificial Intelligence graduate, I am passionate about developing AI models and leveraging data to solve real-world  problems.  My  project  experience  includes  practical  applications  like  an  exam  test  for  pharmacists  and computer  vision  solutions  for  safety  compliance,  such  as  detecting  helmet  usage.  I  thrive  in  collaborative  team environments, enjoying theoretical discussions on optimizing algorithm selection for practical implementation.'},\n",
       " {'id': 'doc1#chunk3',\n",
       "  'text': 'Huachiew Chalermprakiet University Bachelor of Artificial Intelligence ( GPAX 3.89 )'},\n",
       " {'id': 'doc1#chunk4',\n",
       "  'text': '- Programming Languages: Python, JavaScript, TypeScript, HTML, CSS, Shell Script\\n- Frameworks &amp; Libraries: YOLO, Hugging Face, Pandas, Scikit-Learn, TensorFlow, PyTorch (CUDA), LangChain, OpenCV, Next.js, React.js, Flask, Pyspark\\n- Databases: MySQL, PostgreSQL (via psycopg2)\\n- Tools &amp; Platforms: Git, GitHub, VS Code, Canva, Postman, Microsoft 365 (Word, Excel), Power BI, Ollama\\n- Languages: Native Thai, Basic Communication English'},\n",
       " {'id': 'doc1#chunk5',\n",
       "  'text': '- Collaborated  with  a  two-person  team  to  develop  a  PHARMACIST  DISPENSE  SKILL ASSESSMENT PROGRAM (PDSAS).\\n- Integrated the system for deployment to pharmacy faculty, facilitating practical application of the research.\\n- Achieved a fine-tuning accuracy of 0.0005 with the paraphrase-multilingual-MiniLM-L12-v2 model, and demonstrated an accuracy of 5.8042 on new datasets.\\n- Built  the  system  using  Python,  PostgreSQL,  Flask,  and  Next.js,  leveraging  sentence similarity techniques to compare spoken content against scoring criteria.'},\n",
       " {'id': 'doc1#chunk6',\n",
       "  'text': 'Aug 2024 - Nov 2024\\n\\n- Developed a PPE (Personal Protective Equipment) Detection system, from model training to web application deployment.\\n- Trained an AI model using YOLO on a custom dataset, achieving a mAP (mean Average Precision) of 0.868 .\\n- Built  and  integrated  a  web  application  with  React  (frontend)  and  Flask  (backend)  to showcase the AI system.\\n- Implemented AI processing in the Flask backend, including object detection and person recognition with FaceNet.\\n- Explored mobile development concepts using Flutter and Android Studio.'},\n",
       " {'id': 'doc1#chunk7',\n",
       "  'text': '- DIY 6 DOF Robot Arm (2023) - Assembled and programmed a 6-Degrees-of-Freedom robot arm using C++ in Arduino IDE for a one-day workshop to teach younger students.\\n- From Gen Z to CEO (2023) -  Achieved  Top  500  recognition in a \"Gen Z CEO\" aptitude test, gaining insights into business aspects like production, logistics, and marketing.\\n- Student Committee (2023) -  Managed financial tasks, including collecting receipts for reimbursement of communal funds, and supported various committee activities.\\n- Game Jam HCU (2022, 2023) -  Collaborated in a team to develop games within 2-3 days, gaining practical experience with Unity and Godot.\\n- MEA Hackathon (2021) - Participated in the event and presented innovative concepts.'},\n",
       " {'id': 'doc1#chunk8',\n",
       "  'text': 'AI Engineer / AI Developer Data scientist / Data analyst\\n\\n<!-- image -->\\n\\n<!-- image -->\\n\\n0616046040\\n\\nsittikorn720@gmail.com\\n\\ngithub\\n\\nlinkedin\\n\\nTheparak Road, Bang Sao Thong Subdistrict, Bang Sao Thong District, Samut Prakan Province\\n\\n<!-- image -->\\n\\n<!-- image -->\\n\\n<!-- image -->\\n\\n2021 - 2025'}]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "formatted_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c5ce7146",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from pinecone import Pinecone\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "pc = Pinecone(api_key=os.getenv(\"PINECONE_API_KEY\"))\n",
    "index = pc.Index(\"production\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "594725d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UpsertResponse(upserted_count=8, _response_info={'raw_headers': {'date': 'Mon, 01 Dec 2025 07:43:24 GMT', 'content-length': '0', 'connection': 'keep-alive', 'x-pinecone-request-lsn': '1', 'x-pinecone-api-version': '2025-10', 'x-envoy-upstream-service-time': '1064', 'server': 'envoy'}})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.upsert_records(\"markdown-namespace\", formatted_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "142f8363",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv (3.12.4)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
